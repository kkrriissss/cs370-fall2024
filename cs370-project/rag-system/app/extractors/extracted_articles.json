[
    {
        "title": "Getting started with ROS2 \u2014 Part 1",
        "content": "Sign up Sign in Sign up Sign in Sharad Maheshwari Follow schmiedeone -- 1 Listen Share Note: This series assumes that you have working knowledge of ROS1. We focus on getting to know ROS2, which is what the robotics community is moving towards. Here are Part 2 and Part 3 of this series.If you\u2019re new to Robot Operating System as a concept, check out this post to read what ROS actually is. Hello there! Since you already have working knowledge of ROS1, for brevity, we\u2019re gonna jump straight to the differences between ROS1 and ROS2. 1. Third party middleware ROS1 has its own middleware which uses a network stack to send information from a node to the right socket, which is received by the middleware again to send information to the right node (over-simplification).ROS2 ditched the middle and now uses Data Distribution Service(DDS), a third-party middleware that is a reliable industry standard. 2. No Rosmaster Rosmaster is a service facilitating two nodes to find each other (if they wish to communicate). This is followed by them connecting to each other directly. If the rosmaster dies during the process, these nodes will still be connected, but no new nodes can join the communication network. In ROS1, each node needs to know the location of the master (even when the system is run on multiple machines). ROS2 did away with the rosmaster, and nodes can find each other directly. 3. Shared codebase for Cpp and Python ROS has support for two languages, Cpp and Python and the underlying implementation differs in places. For instance, each subscriber has its own thread in Python, but not in Cpp.All that is true for ROS1, but ROS2 has the same shared C implementation. This common codebase is exposed through ROS Client Libraries APIs in C++ and Python. 4. Multiple ROS nodes in a processROS1 does not allow multiple ROS nodes in one process. That changes in ROS2 5. Python build requirementChanging python files in ROS1 doesn\u2019t require you to build the project. ROS2 needs you to build the project again, which suuuuuuuucks! \ud83d\ude26 6. Action ServicesAction Services weren\u2019t a part of core ROS1 libraries (but actionlib). ROS2 core libraries have action servers 7. Windows supportROS2 is supported by Windows! Well, I sincerely hope you\u2019re using Linux though \ud83d\ude1b 8. Changed launch infrastructureROS1 primarily uses XML-based launch files. ROS2 has a python launch system supporting ordering constraints This is a hard question. There is some inertia for the move, mainly because ROS2 community is still not as big as ROS1. This leads to less online development support and longer development time. But most prominent open-source ROS1 packages are or will be at their end of life support soon. Future-proofing projects and upskilling are the two reasons to make the move. Phase 1: Installing ROS2 We are using ubuntu Focal (20.04.3 LTS) in this lesson, which supports ROS2 Galactic.Here\u2019s a link that explains ROS2 installation really well. They do a better job than me to explain installation :) Phase 2: Test ROS2 To test our ROS2 setup, we do the following:1. Source the terminal As mentioned before, I will use galactic instead of $ROS_DISTRO henceforth 2. Run demo listener and talker nodes:We will use demo_nodes_cpp package which is already present in \u201c/opt/$ROS_DISTRO/lib/\u201d Run the following command in the sourced terminal: You will see the following: If yes, we can be sure about our ROS2 setup.Woot woot! We\u2019re ready to start developing in ROS2. Let the game begin \ud83d\ude09Head over to Part 2 in this series here. Stay Awesome,Sharad -- -- 1 Using this space to share stories from our workstation. How did we implement what and why. Building Robots Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/schmiedeone/getting-started-with-ros2-part-1-d4c3b7335c71"
        }
    },
    {
        "title": "A Beginners guide to ROS2",
        "content": "Sign up Sign in Sign up Sign in SKG Labs Follow -- Listen Share ROS 2 is a distributed and modular framework for developing robot software. It provides tools, libraries, and conventions for building complex robotic systems. ROS 2 supports multiple programming languages, making it accessible to a wide range of developers. Before we start coding, let\u2019s cover some key concepts in ROS 2: Terminal 1 (Run Publisher):bashCopy cod Terminal 2 (Run Subscriber): You should see the subscriber receiving messages from the publisher and displaying them in the terminal. Congratulations! You\u2019ve created a simple ROS 2 project with a publisher and subscriber nodes. This tutorial provides a basic understanding of ROS 2 concepts and how to create simple nodes for communication. To learn more about ROS 2, you can explore the official ROS 2 documentation, tutorials, and community resources available on the ROS 2 website. Remember that ROS 2 is a powerful framework, and there are many more features to discover and explore as you advance in your robotics development journey! -- -- We develop all things Robotics, Based in SF Bay Area. Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@tetraengnrng/a-beginners-guide-to-ros2-29721dcf49c8"
        }
    },
    {
        "title": "Distributed System in ROS2",
        "content": "Sign up Sign in Sign up Sign in Markus Buchholz Follow -- 1 Listen Share When you run your ROS2 application, there are often requirements to run ROS2 nodes in different locations on different machines. Since ROS2 uses in the abstract DDS layer for communication between the nodes, we can arrange the communication very easily. In order to understand fully the architecture of ROS2 I recommend you to be familiar with this article. For the purpose of this article, I communicated the Nvidia Jetson Xavier and my PC. Both units run ROS2 Foxy. I connected the RGB camera to Jetso and ran a publisher (in C++ or Python). The PC runs subscribers (C++ or Python) to subscribe to the topic and displays a video stream. The source code is available on my GitHub. The architecture can be depicted as follows. Below I include, the ROS2 command to run your application according to the above figure. The application can be run on the same machine or other configurations without changes. The most important however is to run the application (publisher \u2014 subscriber on the same ROS_DOMAIN_ID. In order to align you have to export the same ID. If you run your application in C++, Modify your CMakeList.txt (see also my Github). Here I display CMakeList.txt to build publisher and subscriber. You have to choose your machine type. Copy the pub_cam.cpp and sub_cam.cpp (from my repository) to /dev_opencv/src. If you run your application in Python, Copy Python scripts to dev_opencv_py and modify setup.py, In package.xml change Run following command, Thank you for reading. -- -- 1 Researcher in underwater robotics Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@markus-x-buchholz/distributed-system-in-ros2-d3d06cf45157"
        }
    },
    {
        "title": "Reinforcement Learning Path Planner for 6DOF Robot in ROS2",
        "content": "Sign up Sign in Sign up Sign in Markus Buchholz Follow Geek Culture -- Listen Share In this article I will give you example how to create robot simulation in ROS2 (Foxy). Doosan collaborative robot will be a the part of our project. We are going to use reinforcement learning agent to find an optimal path. We use numerical Inverse Kinematics to compute joints position of the robot. Finally set of robot joints will be sent to robot controller to perform motion avoiding obstacle. Robot will move according to path planner (which find the optimal path between point A and B avoiding obstacle). There are many algorithm which finds optimal path. Usually the optimal path for the robot movement is search by for example by A* algorithm (see my article and implementation in C++) which finds the optimal path in joint space coordinated space. In this article we are using the Cartesian space. The reinforcement learning agent use Bellman equation for path computation (see my previous article). For the sake of simplicity, in this project the the path is computed in 2D (XY), Z plane is considered as constant. The idea of reinforcement learning I depicted in my previous article. Here this method is reused. Considering Z axis as constant, the agent looks for optimal path in 3x3 maze. Please note, in src folder you will fine implementation of 3D planner where the agent interact with 27 states (3x3x3 matrix).Each point of the maze is considered as a XYZ position. So for 3D the maze in our case can be represented as a cubic with 27 nodes (as I mentioned above in this project we consider that Z axis as constant so our agent finds the way in 9 space environment).When the way to the goal robot position is satisfied (the path between start and goal). The positions are send to inverse kinematics manger in order to compute (numerically) the joint potion of the robot (for the node in maze). Inverse kinetics Manager computes robot position for specific orientation but you easily change (consider the comments in the code). The simulation can be seen in Gazebo. Consider below \u201csignal\u201d flow between ROS2 nodes.The project is a simplification of general robot motion planning problem . In reality the joint space for the robot is modeled by thousands or millions of nodes covering the working space of the robot. In this project we use simplified approach to be familiar with this algorithm which can be used as a path planner. For the environment with many nodes (position of the robot) we apply Deep Reinforcement Learning, where our Q-table (brain of robot) is replaced by neural network (see may previous articles about deep reinforcement learning). In following section I will depict steps which allows you to build regarding ROS2 simulation from scratch. You will find also the full working packages on my GitHub.Note. I am not going to preset the architecture of ROS2 and how it works under the hood. There is excellent documentation: ROS2, Moveit, Control.I recommend also the check URDF in Gazebo tutorial. At the beginning, create you working space. Here I created robotPP (after \u201ccolcon build\u201d command) As you can see on the below image, in src there are 4 folder (packages) which you are going to create step by step.ai_path_planner_pkg is a main package where you start your main function.gazebo_ros2_control is a package which is responsible for controlling robot we are going to use. This package we are going to clone from official repository.ik_interface is a service package which manages the data between the nodes. In this project main program in (ai_path_planner_pkg) communicates with it ik_manager throughout service package (in this case ik_iterface).Please check file in ik_interface/srv/XYZJoints.srv .The first three lines are the parameters of the request (send to manager), and below the dashes is the response (computed joint). robot_ik is a \u201cmanger\u201d of robot inverse kinematics. The manager computes numerically robot joints. Input for these computations are XYZ positions XYZ which are \u201cfounded\u201c by reinforcement learning agent. XYZ positions and computed joints are transported throughout defined above service (IK_interface) In src folder perform following \u201cactions\u201d: Running above command you should have similar folder setup on your machine. Please take a look on my gitHub in order to check the project setup. Now I will focus on ai_path_planner_pkg package since it includes definition of the robot, launch files and control setup.The folder looks as follows Folder /description includes all file necessary to \u201cbuild\u201d a virtual model of the robot. Sub-folders and files includes are generated by Doosan robotics (see here). Launch folder includes the python script which start Gazebo simulation. In python script we call first xacro file which defines robot (robot kinematics and physical parameters like: mass, inertia, type of links, velocity etc ). In order to influence the robot motion, you have to call controller (see macro.gazebo_config_control.xacro) which as you can imagine can be considered as interface between the command to robot (like moveL or moveJ) and physical robot (here the simulation in Gazebo). Controller takes commands and computes joint trajectories in order to achieve targets. Now you have to build and source: Now open your terminal (or terminator) and run following commands (run command in separate instance of terminal) Expected movement of the robot. References: [1] David Valencia [2] Robotics Casual -- -- A new tech publication by Start it up (https://medium.com/swlh). Researcher in underwater robotics Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/geekculture/reinforcement-learning-path-planner-for-6dof-robot-in-ros2-518581dc72c7"
        }
    },
    {
        "title": "Robotic Projects: Reasons for Switching from ROS2 to ROS1",
        "content": "Sign up Sign in Sign up Sign in Sebastian Follow -- 1 Listen Share The robot operating system ROS is the most widely used robotic middleware platform. It\u2019s being used in the robotics community for more than 10 years, both in the hobbyist area as in industry. ROS can be used on a wide array of microcontrollers and computers, from Arduino to Raspberry Pi to your Linux Workstation, and it offers hardware support for motor controllers, visual sensors, depth cameras and laser scanners. In my project to build a robot, I worked with both ROS1 and ROS2. So far, I finished 2 out of 5 phases. The first phase was research: Understanding the hardware and software of robotic, researching microcontroller and sensor, testing libraries and middleware. In the second phase, I built an Arduino based prototype, an IR controlled 4-wheel vehicle that I termed RADU. Now, Phase 3 has two distinct parts: Simulate the robot with ROS, then build a ROS-enabled robot. And this is the dilemma: The simulation with ROS2 \u2014 hard earned with long trial&error periods \u2014 could not be extended to the concrete robot because my hardware is not supported. For a long time, I considered this, and finally decided to continue with ROS1 from hereon. This article summarizes my experiences, and explains the rationale for the final choice. This article originally appeared at my blog admantium.com. When starting the project at the beginning of 2021, my first experience with ROS was confined to pure simulation. With an Ubuntu base Linux running as a VM on my laptop, I installed ROS noetic with RViz and made a simple URDF model of a four wheeled robot. Then, I bought a robot chassis, several Arduino Uno, and 5V compliant sensors. Working through the tutorials, understanding how Arduino works and how sensor data is measured and processed, was an invigorating time. I had many \u201ceureka\u201d moments in which after some difficulties the sensor would suddenly work. This phase finished with a first concrete prototype, a moving 4-wheel chassis controlled by an IR. Preparing the full utilization of ROS in my project, I installed ROS2 on a dedicated Linux Workstation. Then I began to port my simple RViz ROS1 model to ROS2, and extend the model to be compliant with Gazebo. On the fun side was learning the physics behind robots, and getting deep knowledge of ROS concepts. On the downside, I spend long periods of trial and error to develop a Gazebo compatible model. But finally, I could launch the simulation, start a teleop node, and move the robot in a simulation streaming point cloud sensor data. From simulation to robot was the next phase. In the meantime, the Raspberry Pico entered the stage. Initially I just tested how to add sensors and read them with C and MicroPython. But reading several tutorials and watching videos around Raspberry Pi based projects with robot control software in Python convinced me of the feasibility. Writing the motor control software from Scratch in MicroPython became my goal. I rebuild my Robot completely with MicroPython, and added also a ROS Twist Message to motor control commands wrapper. The robot moved! But when adding my Realsense D435 camera, I could not get point cloud data streamed with ROS2, after several hours of trying. Out of curiosity, I switched to ROS1 \u2014 and here it worked out of the box! This revelation gave rise to the observations summarized in the next paragraph. ROS2 is the ROS of the future. According to information from community websites and YouTube videos, ROS2 is geared towards industrial use. Robot communication uses a standard middleware layer called XRCE-DDS. ROS2 also inhibits feature of system self-healing and reliability, such as no ROS master node is required to run. When looking into concrete robot projects, ROS2 robots from the community pale against ROS1 robots. Most projects developed in the last 3 years still work with ROS1. I did not consider this fact, merely felt astonished because ROS2 was already released since 2017. After working with ROS2 for simulating a robot and for getting ROS2 to run with my robot sensors, particularly the RealsenseD435 camera, several obstacles occurred to me. This is a personal recollection. First, when searching for how to connect a certain sensor, ROS2 related information, concrete articles, documentation, or YouTube videos, is hard to find or non-existent. Second, documentation about special topics such as Gazebo controllers and Gazebo plugins is incomplete. If you want to build a Gazebo simulation, there are explorative and comprehensive manuals for ROS1, but for ROS2, you need to fetch information pieces from different websites, assemble them, and use a lot of trial and error before it works. Getting my robots simulation to work with all sensors and topic in Gazebo/RVIZ 2 was a long journey. Third, when assembling a robot by using an SBC and connect it to microcontrollers, you will be surprised to find out that there is no universal library for connecting them via serial, as explained in this blog post. You either need to use very specific microcontrollers, and compile a custom RTOS Linux distribution to them, making it hard to reuse your e.g. Arduino libraries. Or you need special board with networking features. Fourth, checking hardware support for ROS2 shows that fewer options are available and the other robot enthusiasts have problems to get ROS2 running. I had the very same experience when trying to get the Intel Real SenseD435 work with ROS2. Although I used the latest firmware, compiled the librealsense from scratch, compiled the ros-realsense package, it did just not work and even a thread on the very responsive and helpful community issue board did not find an answer. When I switched to ROS1, the same sensor just works without any problems. Generalizing these observations leads me to the sad conclusion that using ROS2 means to find fewer documentation, have fewer support for sensors, and harder integration of SBCs and microcontroller. The experience in getting the gazebo Simulation to work in ROS2 was mostly trial and error. Getting the Realsense2 camera to work in ROS2 was not successful. Thereby, reading from other about similar errors and problems with ROS2, was another impact. And finally, when researching about the next aspects of using ROS on my robot, like SLAM, autonomous movements, gripper \u2014 did reveal time and time again complete tutorials and books in ROS1, I made the decision: My project will be continued with ROS1. -- -- 1 IT Project Manager & Developer Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@admantium/robotic-projects-reasons-for-switching-from-ros2-to-ros1-9f8342be348f"
        }
    },
    {
        "title": "Building a ROS 2 Project \u2014 Part 1",
        "content": "Sign up Sign in Sign up Sign in Sharad Maheshwari Follow -- 1 Listen Share Note: Checkout Part 0 to understand how this series is set up. After this series, we\u2019ll know how to design, set up, and maintain a dummy robotics project using ROS2, Docker, and Github. Hello again! In part 0, we described our problem statement \u2014 Build a Trash Collection Robot in ROS 2. Well, not literally. We use this problem statement to build a boilerplate Trash Picking Robot in ROS2 to understand how to set up an application-based ROS2 project. Our focus is not on actually building a robot, or writing high-quality code (yet). We aim to understand how to set up a ROS2 project in software and interact with different nodes for a specific application. Hence, our design is over-simplified for demonstration. Let\u2019s begin, shall we? \ud83d\ude00 We aim to build a Trash Collecting Robot that looks at its surroundings, moves to trash, and collects it. Simplification \u2014 As someone learning ROS2, we have a constraint \u2014 No access to relevant hardware. Thus, we simplify our problem statement to the following \u2014 How do we design this ROS2 project which serves as a software boilerplate we can build on later? We simplify the design to software boilerplate and focus on brain and perception packages described in Part 1. Actuation package will be an exercise for you. Let\u2019s begin by building perception package. We\u2019ll list each step, big or small. Note:1. Always source ROS2 in each terminal \u2014 source /opt/ros/galactic/setup.bash2. Always source your workspace in each terminal \u2014. install/setup.bash Failing to do this will result in errors Step 1 \u2014 Create a new ros2 workspace Step 2 \u2014 Build the workspace At this point, you will see \u201c0 packages finished\u201d in the terminal (because there are no packages yet). Step 3 \u2014 Create a new perception package (we use python) This will be the output \u2014 We have an empty package now! Step 4 \u2014 Navigate to the code directory (where we will place our node file) Note: this directory has the same name as the package Step 5 \u2014 Make perception node (or vscode, or vim, or whatever you like. I use vscode \ud83d\ude03) Let\u2019s look at the main elements of perception_node.py \u2014 Step 6 \u2014 Add dependencies Navigate back to find package.xml and add the following lines \u2014 Our package depends on rclpy and std_msgs. Step 7 \u2014 Add an entry point Open setup.py and add the following lines within the console_scripts bracket of the entry_points field: Step 8 \u2014 Build and run Run rosdep in the root of your workspace (ros2_ws) to check for missing dependencies before building. build the workspace (from ros2_ws) \u2014 source the terminal(from ros2_ws) \u2014 Note: In ROS1, this was equivalent to \u2014 source devel/setup.bash Step 9 \u2014 Run the node If everything was done properly, we see the following error \u2014 Whaaaaaaat? We said before that we need a custom service called ComponentStatus , which according to this codebase should be made inside a ROS2 CMake package we name custom_interfaces . Unlike ROS1, there currently isn\u2019t a way to generate a .msg or .srv file in a pure Python package. You can create a custom interface in a CMake package, and then use it in a Python node So we should make a CMake package separately we call custom_interfaces . We will define our service ComponentStatus here. Step 1 \u2014 Create a new custom_interfaces package (CMake) Step 2 \u2014 Create custom service ComponentStatus Add the following to ComponentStatus.srv \u2014 This service expects component name as input and status of the component as output (all string) Step 3 \u2014 Update CMakeLists.txt in custom_interfaces package Add the following lines to CMakeLists.txt Step 4 \u2014 Update package.xml in custom_interfaces package Add the following lines to package.xml Step 5 \u2014 Build custom_interfaces package Now the interfaces will be discoverable by other ROS 2 packages. Step 6 \u2014 Confirm srv creation \u2014 In a new terminal, run the following command from within your workspace (ros2_ws) to source it: Now you can confirm that your interface creation worked by using the ros2 interface show command: You should see the following \u2014 Your custom service ComponentStatus in the package custom_interfaces is ready for use \ud83d\ude00 Step 7 \u2014 Run perception node again You will not see any errors now because perception could find ComponentStatus definition this time. To see if perception is processing information, we publish \u201ctrash\u201d on camera topic, which mocks an image with trash. If perception works properly, it will process this string and publish \u201cI see trash\u201d on trash_detection topic. To publish on camera topic once, open a new terminal, source ROS2, and run the following command \u2014 See what perception does \ud83d\ude03 Now that our perception is alive, it\u2019s time to build our brain package, which is the central decision-making node! Step 1 \u2014 Create a new brain package (we use python) Step 4 \u2014 Navigate to the code directory (where we will place our node file) Step 5 \u2014 Make brain node Let\u2019s look at the main elements of brain_node.py \u2014 Let\u2019s talk about something new here \u2014 Multithreading.Brain Node has 2 callbacks \u2014 trash_detection_callback for trashDetectionSubscriber and send_component_status_request for componentStatusServiceTimer By default, they are run in the same thread, which can cause deadlock. That means if we use the same thread for both, it will be busy processing only trash_detection_callback , and send_component_status_request will never be called. As soon as a service request is sent from brain , the node comes to a halt due to deadlock. To avoid this, we register these callbacks in separate threads for concurrent processing. But how do we do this? Using MultiThreadedExecutor and ReentrantCallbackGroup \u2014 Line 54/55 \u2014 MultiThreadedExecutor is responsible for running all callbacks in brainNode in appropriate threads, which is decided by the callback_group each callback is registered in. Line 14/21/27 \u2014 ReentrantCallbackGroup is one of the callback groups available in ROS2. All callbacks registered in this group run in separate threads. Just for fun \u2014 Try commenting out lines 21 and 27 to disable concurrent callback processing after you\u2019re done with the lesson. Brain node will not run properly with just one thread \ud83d\ude03 Step 6 \u2014 Add dependencies Navigate back to find package.xml and add the following lines \u2014 Our package depends on rclpy and std_msgs. Step 7 \u2014 Add an entry point Open setup.py and add the following lines within the console_scripts bracket of the entry_points field: Step 8 \u2014 Build and run Run rosdep in the root of your workspace (ros2_ws) to check for missing dependencies before building. build the workspace (from ros2_ws) \u2014 source the terminal(from ros2_ws) \u2014 Step 9 \u2014 Run the node If everything is done correctly, brain node will keep on waiting for ComponentStatus service. Once we also run perception node, this will go away and the complete system will be ready. To see our dummy robot in action, run perception and brain node in separate terminals (remember to source ros2 on each ) \u2014 Terminal 1 \u2014 Terminal 2 \u2014 Viola! The two nodes are now connected and ready to process camera topic data (\u201ctrash\u201d/\u201dno trash\u201d mock messages) Additionally, we see component status service request from brain every 5 seconds, which perception serves. Here\u2019s how it looks. Terminal 1 \u2014 Terminal 2 \u2014 Now let\u2019s publish a mock image (string for demonstration purposes) on camera topic. While brain and perception are running, open a third terminal and publish \u201ctrash\u201d on camera topic We will see \u201cPublishing: \u201cI see trash\u201d on perception\u2019s terminal. Perception processed information on camera topic and inferred the presence of trash in the mocked image. This information is passed to brain. Since perception published \u201cI see trash\u201d on trash_detection topic, brain decided to move the robot. We thus see \u201cSending move request to actuator\u201d on brain\u2019s terminal. Try publishing \u201cno trash\u201d on camera topic. We see \u201cSending no request to actuator\u201d on brain\u2019s terminal. To constantly publish the string (\u201ctrash\u201d/\u201dno trash\u201d) on camera run the following on a new terminal \u2014 We will see brain constantly asking the robot to move, as it receives data from perception. The last piece here is sending move request from brain to actuation . And that\u2019s an exercise for you. All you need is three simple steps \u2014 To sum it up, we created brain and perception packages for our Trash Collecting Robot. So here we are, with our complete project! (We skip actuation node implementation for brevity, but encourage you to do so). Next up, we use ROS2 parameters and launch files in Part 2 to improve project execution. See you there! Ciao,Sharad -- -- 1 Building Robots Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@thehummingbird/building-a-ros2-project-part-1-a2c02d6ac3d8"
        }
    },
    {
        "title": "Getting started with ROS2 \u2014 Part 2",
        "content": "Sign up Sign in Sign up Sign in Sharad Maheshwari Follow schmiedeone -- Listen Share Note: This post is the second part of our series (duh!). Sorry! What I mean is, checkout part 1 to get a better hang of stuff here \ud83d\ude03Part 3 also available. (ROS1 working knowledge assumed) Aim \u2014Understanding publisher, subscriber, service, and service client creation in ROS2.Step 1 \u2014 Create a custom publisher-subscriber package, Step 2 \u2014 Create a service-client package Remember when we started learning ROS1 and made a custom package with a publisher and subscriber pair? One node publishes and the other constantly listens over a topic. Let\u2019s do that again! 2. Build the workspace ROS1 uses catkin, but ROS2 uses colcon to build packages At this point, you will see \u201c0 packages finished\u201d in the terminal (because there are no packages yet). Additionally, you will now have build, install and logs folders in the ros2_ws. 3. Create a new talker package (we use python) Similarly, we can also make a cpp package. Please look at ROS2 official guide. This will be the output \u2014 We have an empty package now! 4. Navigate to the code directory (where we will place our node file) Note: this directory has the same name as the package 5. Make the talker node (or vscode, or vim, or whatever you like. I use vscode \ud83d\ude03) Okay, we need to talk (no pun intended). Here is some information to start understanding this code and compare it with ROS1. I mentioned in the previous post that rclpy has APIs to use the underlying C codebase in ROS. And that\u2019s what we see here.rclpy has APIs and classes to abstract a lot of stuff we did ourselves in ROS1. Node class abstracts node initialization, creating publishers, subscribers, services, clients. Here, we inherit all methods and properties from Node. \u2014 We don\u2019t initialize the node ourselves \u2014 We use \u201ccreate_publisher\u201d method from Node (line 11) to make a publisher \u2014 We use create_timer method from Node (line 13) to use its timer and set a callback method. We used rate to do something like this in ROS1 \u2014 We can just spin the node and our timer will lead to the callback based on the set period. Note: rclpy.spin is blocking Please play with the code to understand more. In case of any issues, feel free to comment and I\u2019ll make the post more verbose. 6. Add dependencies Navigate back to find package.xml and add the following lines \u2014 Our package depends on rclpy and std_msgs. 7. Add an entry point Open setup.py and add the following lines within the console_scripts bracket of the entry_points field: 8. Check setup.cfg setup.cfg should be set automatically to this \u2014 This is simply telling setuptools to put your executables in lib, because ros2 run will look for them there. 9. Build and run You likely already have the rclpy and std_msgs packages installed as part of your ROS 2 system. It\u2019s good practice to run rosdep in the root of your workspace (ros2_ws) to check for missing dependencies before building. build the workspace (from ros2_ws) \u2014 source the terminal(from ros2_ws) \u2014 Note: In ROS1, this was equivalent to \u2014 source devel/setup.bash 10. Run the node If everything was done correctly, this will be our terminal \u2014 Woot woot! Our first custom package in ROS2 is alive! Let\u2019s make a listener. As an exercise, please follow steps 5 through 9. Here\u2019s the code for listener_node.py (in the same directory as talker_node.py) \u2014 We just need to add one more line in setup.py for this package. The new file will look like this \u2014 11. Build and run You likely already have the rclpy and std_msgs packages installed as part of your ROS 2 system. It\u2019s good practice to run rosdep in the root of your workspace (ros2_ws) to check for missing dependencies before building. build the workspace (from ros2_ws) \u2014 source the terminal(from ros2_ws) \u2014 We are almost done. If the build was successful, we run both the nodes in different terminals. Terminal 1: Terminal 2: Voila! We have written our first ROS2 package with a publisher and subscriber. Now, let\u2019s start fresh in the same workspace (ros2_ws) and create a package with a server node to add two integers and a client node to request this service. 1. Create a new add_integers package We use \u201cdependencies\u201d this time, which automatically updates package.xml (unlike the last time when we did this ourselves) example_interfaces package has a simple service definition that we will use. It looks like this \u2014 Later on, we make a custom service. This will be the output \u2014 We have an empty package now! 4. Write the service node \u2014 Note: As I mentioned in the previous post, Node class abstracts a lot of things we did ouselved in ROS1 (with its constructor and APIs like create_publisher, create_subscriber, create_service,etc)Please play with the code to understand more. In case of any issues, feel free to comment and I\u2019ll make the post more verbose. 5. Add an entry point Open setup.py and add the following lines within the console_scripts bracket of the entry_points field: Let\u2019s also create a service client(which sends two numbers using cmd arguments) and build both together. 6. Write the client node Here\u2019s the code for client_node.py (in the same directory as listener_node.py) Let\u2019s have a quick chat, shall we? While we\u2019d be able to understand most of the stuff in the client code here, there are two new ideas here (compared to ROS1)1. Asynchronous client call \u2014 Line 20 uses an async call, which does not block this thread. If we use call(), which is synchronous, this thread will be locked until a response is received from the service, causing potential deadlocks2. Future \u2014 Line 31 uses future, a value that will be updated when the service has done its job. We can check this value anytime after the call to see if we have a response. Both the points above are explained beautifully here. 7. Add an entry point Open setup.py and add the following lines within the console_scripts bracket of the entry_points field: 8. Build and run You likely already have the rclpy and std_msgs packages installed as part of your ROS 2 system. It\u2019s good practice to run rosdep in the root of your workspace (ros2_ws) to check for missing dependencies before building. build the workspace (from ros2_ws) \u2014 source the terminal(from ros2_ws) \u2014 Run the service node (terminal 1) \u2014 Run the client node with two numbers as arguments (terminal 2) \u2014 Here\u2019 what the result will look like \u2014 Woot woot! Our package add_integers with a service node and a client node is up and running! Interesting Fact: ROS1 did not require the creation of a node to request for a service. We could use client APIs without a node and it would work just fine. Although I still need to confirm this, but we possibly do need a ROS2 node for our client. This is based on my observation that client creation APIs area a part of Node class now. Thus, we make a node. And with this, we wrap up our post. We look at parameters and actions in our next post here! Stay tuned (and stay awesome)!Sharad -- -- Using this space to share stories from our workstation. How did we implement what and why. Building Robots Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/schmiedeone/getting-started-with-ros2-part-2-747dd63bdcb"
        }
    },
    {
        "title": "Robot Operating System: Getting Started with Simulation in ROS2",
        "content": "Sign up Sign in Sign up Sign in Sebastian Follow -- Listen Share In a robotics project, simulation is an important aspect that serves multiple purposes. First, you can test the behavioral code that you want your robot to execute. Second, you can use the simulation to test different types of hardware, for example distance sensors, cameras or 3D point cloud sensors to see which works best. Third, the same software that visualizes a simulation can be used in real time, with a real robot, to see the environment while its being scanned and navigated by the robot. In my robot project, I started with ROS1 and provided a basic robot model that can be visualized with RVIZ. Then, I build a first practical prototype, entirely Arduino-based with no ROS involved. In the current phase of the project, I work on building a Gazebo compatible model with visual sensors. This article continues the series with a jumpstart into ROS2 simulation with Gazebo. We will get to know the essential tools for creating a simulation \u2014 ROS launch files and Gazebo \u2014 and learn to apply the essential steps \u2014 choosing 3D models, placing them in the world. The technical context for this article is Ubuntu 20.04, ROS2 Foxy, Gazebo11, but it should work with newer versions too. This paragraph shortly lists all the terminology that is essential to understand the physical simulation aspects. Skip this section if you are firm with the vocabulary. Source: ROS Robot Programming Book Source: wikipedia.org Gazebo was started outside of ROS, but became fully integrated. It focuses more on the complete physical simulation of a robot and a world. Particularly, the world provides correct physical simulation through a physics engine: The robot can bump into objects, which will move and eventually even crash on your robot. Gazebo comes with predefined world models, and you can even define your own model. In this tutorial, we will launch a Gazebo simulation with an empty world, and then spawn a robot inside. In a nutshell, the essential steps are: Each following section details these tasks. We will create a package structure that looks as follows: For convenience, just run the following commands: The room.world file is an SDF file that will contain <model> tags of everything we want to simulate: Objects like walls, windows, and furniture. We will start with a simple empty world and new objects add step-by-step. You can load this file into Gazebo manually. But since we will ultimately also spawn a robot in this model, its best to continue with the launch file straight ahead. As we learned in the last article, ROS2 does not support XML launch files anymore, and instead Python files are used. The launch file that we use wraps the launch command from the gazebo_ros package, and provide a world argument. In order to use our URDF model from RViz in Gazebo, we need to make several changes to the model. First of all, we need to provide additional physical aspects to the robot so that it behaves correctly in a simulation. Second, the visual appearance of the robot differs. The color and text definition of an RVIZ model is not applicable. If you use meshes, they need to be changed as well. Third, we also need additional plugins so that the Gazebo tool works properly Lets review these changes step-by-step. Inertia The inertia of an object is the counterforce that it applies when its current motion is influenced by another object. In Gazebo models, the <inertial> tag is used to represent this aspect. Here is an example: The <origin> tag positions the link with respect to its parent link, here you would change the xyz values to move the estimated mass center. With <mass> you specify this links mass in kg. Finally, the <inertial> element is a matrix of how forces on x, y, and z are affecting the link. You can read the physics article on wikipedia, or use this handy python script. Friction Another set of variables governs the friction of links in your robot. You express this with four values. First, <kp> and <kd> provide the static and dynamic contact stiffness. Here, we use the values 100000 and 1.0, which is the default value used in many ROS projects. Second, the values <mu1> and <mu2> are the static and dynamic friction coefficients, which you can lookup on wikipedia based on the material of the link. Collision This defines the hard material boundaries of your robot \u2014 it affects how gravity and other forces are applied to your robot in the simulation. These properties are expressed with the <collision> tag inside the links. Its properties are very simply: Just copy the original links' <geometry> and <origin> values as follows. Joint Characteristics The joints in your robot should be further modeled to express their real-world behavior. To change the visuals of your robot, you have these options: Simple Colors This works the same as in RViz: Inside your links <visual> tags, you can reference a <color> element. Predefined Meshes Gazebo provides a set of build-in meshes that are listed in this source code file. Apply them be adding an <material> tags inside the <gazebo> tag as follows. Custom Meshes When you use custom meshes to represent your links, simple reference them in the <geometry> tag of your link as follows: Important: It is recommended to not use custom mesh files in the <collision> tags since this will affect the simulation performance. Instead, define the values based on the available <geometry> types of boxes, cylinder and sphere. As you see, the required changes are fundamental. And they are not backwards compatible: All the changes required for Gazebo cannot be parsed by RVIZ. For these reasons, complex robot projects separate the URDF aspects into different XACRO files. After some experimentation, I came up with the following hierarchy. Let\u2019s see how this approach works in practice. When running Xacro to render a model, the command xacro rviz.xacro -o radu_rviz_compiled.urdf` will be used. This file will... 2. Define the essential parameters that control the macro execution 3. Execute the macros to create the URDF model After working with this approach for some time, I realized that the core logic of handling the variability is inside the core.xacro file: The macros for rendering links and joints have different blocks that will be triggered by the master file. See the following definition of a <link>. In line 3, an <xacro:if> condition is evaluated to add RViz-specific visuals. And in Line 14, another condition checks and applies the gazebo physical properties to the model. The Gazebo node is started with a launch file, but the robot needs to be spawn into the node. Thanks to the blog article how to spawn robots in ROS2, I created the following launch file. The launch file can also transform the Xacro files during startup, as shown in the diff_bot example. For example, to load the Gazebo configuration, you need to execute this: First, we build the current workspace. Then we launch Gazebo. And then spawn the robot. Finally, the robot is rendered: This article showed you how to create a Gazebo compatible simulation in ROS2 from scratch. It turned out to be lengthy process in which you need to (1) create a package, (2) create the world file, (3) create the launch file, (4) add Gazebo specific tags to our robot model, (5) parametrize your robot model to be compatible with Gazebo and RVIZ, and (6) spawn the robot entity in the simulation. Form all these steps. adding the Gazebo physics was time-intensive to learn and apply, and I hope you gained valuable insights as well. At the end, we can spawn RADU in Gazebo and RViz with a custom launch script file. And from here, we can move the robot around in its simulation. -- -- IT Project Manager & Developer Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@admantium/robot-operating-system-getting-started-with-simulation-in-ros2-cd7d64268ba9"
        }
    },
    {
        "title": "Tuning the ROS2 Nav2 Stack",
        "content": "Sign up Sign in Sign up Sign in Member-only story Canyon Lake Robotics Follow -- Share If you have a wheeled robot in ROS2, you are almost surely using the Nav2 stack. It provides a ton of great features such as mapping, control, and sensor fusion. However, you are also likely to find that the tutorials and other online references don\u2019t work well on your robot. For example, when your robot turns suddenly, it might become lost. You may also find that the robot is not good at making small adjustments; arriving within, say, 2 cm of your target position seems like a matter of luck more than algorithmic determinism. This article can help. It explains the key parameters which helped our differential-drive robot become much more repeatable in its path following while also finishing the paths faster. Specifically, we used a Rover Zero 3 with ROS2 Humble / Gazebo Fortress. The repository at https://github.com/RoverRobotics/roverrobotics_ros2 was our initial baseline. Here are our recommendations: -- -- Roboticist, Software Developer, Outdoor Adventurer Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@CanyonLakeRobotics/tuning-the-ros2-nav2-stack-5b01f455e217"
        }
    },
    {
        "title": "An Adaptable Approach to Multi-Robot Navigation in ROS2: Utilizing Turtlebot3 and Nav2",
        "content": "Sign up Sign in Sign up Sign in Arshad Mehmood Follow -- 3 Listen Share In this tutorial, we\u2019ll dive into the process of initiating multiple Turtlebot3 robots along with Nav2 stack within the Gazebo simulation environment. Each robot will have its own namespace, ensuring individual control and navigation using either RViz2 or command-line navigate_to_pose action. The methodology employed here is both adaptable and scalable to accommodate any number of robots, limited by system resources. The sole aspect that needs adjustment to alter the robot count is the primary launch file, where the robot configurations are stipulated as list items. To provide clarity, this tutorial assumes that you already have ROS2 Foxy (or a later version) installed, and set up the Turtlebot3 simulation package. If not, please refer to the official ROS2 and Turtlebot3 documentation to get started. I have written an earlier article to show bare minimum multiple turtlebot3 setup for simple drive in gazebo. In this article we will enable Nav2 for turtlebot burger robots in a multi robot scenario. The launch script initiates the creation of four robots, with the provision to modify their count and placement directly within the code. Before proceeding, make sure you have the following prerequisites: To modify the TurtleBot3 models, it\u2019s essential to have access to the source code. In addition, all required dependencies need to be installed to ensure smooth operations. This process can be simplified by using rosdep, a command-line tool from ROS that automatically installs dependencies, thereby preparing a ready-to-go environment. The repositories turtlebot3_simulations and navigation2 have been updated with all the required changes. You just need to clone these repositories, build the workspace, and then execute the launch commnad. Clone repo and install dependencies Console For ROS2 Foxy use foxy branch For ROS2 humble use master branch The code in the \u201cfoxy\u201d branch is compatible with ROS2 humble. In the master branch, there is an updated launch file for bringing up nav2 with composite nodes. However, the creation of composite nodes is currently disabled due to an issue in the ROS2 humble implementation. This issue pertains to the propagation of namespace mapping to nodes (in composite container) with sub-namespaces, such as \u201c/global_costmap/global_costmap\u201d. Build and Run To build and run the Gazebo environment, follow these steps: Gazebo with four turtlebot3 robots Rviz output ros2 node list output As seen above, each robot has it\u2019s own namespace. Launch options The default configuration of the system enables rviz, which results in the opening four rviz windows. However, the rviz launch can be disabled by utilizing the enable_rviz launch option. Similarly, the auto drive nodes are initially disabled but can be enabled by employing the enable_drive launch option. The launch command sets the initial pose using /initialpose topic. After which the user have the option to provide a goal using the Navigation2 Goal menu in rviz for that robot instance. Command line method Setting initial pose via command line for tb1 robot Setting navigation goal via command line for tb1 robot Summary of Changes: Turtlebot3_simulations repo: Navigation2 repo: The explanation for change 1,2 and 3 is the same as given in previous article. Robot Configuration The arrangement of robots can be specified in a launch file. A potential future enhancement could involve retrieving the configurations from a file, such as json. # Names and poses for the robots robots = [ {\u2018name\u2019: \u2018tb1\u2019, \u2018x_pose\u2019: \u2018-1.5\u2019, \u2018y_pose\u2019: \u2018-0.5\u2019, \u2018z_pose\u2019: 0.01}, {\u2018name\u2019: \u2018tb2\u2019, \u2018x_pose\u2019: \u2018-1.5\u2019, \u2018y_pose\u2019: \u20180.5\u2019, \u2018z_pose\u2019: 0.01}, {\u2018name\u2019: \u2018tb3\u2019, \u2018x_pose\u2019: \u20181.5\u2019, \u2018y_pose\u2019: \u2018-0.5\u2019, \u2018z_pose\u2019: 0.01}, {\u2018name\u2019: \u2018tb4\u2019, \u2018x_pose\u2019: \u20181.5\u2019, \u2018y_pose\u2019: \u20180.5\u2019, \u2018z_pose\u2019: 0.01}, # \u2026 # \u2026 ] Single map_server and global /map topic To ensure a coherent system with a single map, the parent launch file instantiates a single map server instance. In the localization_launch.py file, the map server instantiation is suppressed using a newly added launch parameter. This arrangement aligns with the actual scenario, as there is only one map in the system. All nodes subscribe to global /map topic. nav2_params.yaml modified to include map_topic with \u2018/\u2019 (e.g /map) for amcl, global and local costmap nodes. /tf and /tf_static per robot This implementation adheres to the scheme of having each robot with its own tf tree published under its specific namespace (e.g., /tb1/tf) without the use of tf_prefix. Currently, the map server is created individually for each robot, but as a future enhancement, it can be created once for all the robots, optimizing resource utilization and improving efficiency. Scan topic mapping A modification has been made to the navigation_launch.py script to incorporate scan topic remapping. This modification ensures that the global and local cost nodes, created under the robot namespace, can properly adjust and connect to the correct scan source that corresponds to the respective robot instance. Updates to rviz view file In order to use the same view configuration file for each robot-specific rviz instance, the topics mentioned in the rviz view file have been modified. The prefix \u201c/\u201d has been removed from these topics. This adjustment allows the subscribed topics to be associated with the target interface when rviz is launched with the robot\u2019s namespace using \u201c__ns:=<namespace>\u201d. As a result, ROS2 will add the namespace as a prefix to the topic name, since the \u201c/\u201d prefix has been removed. This modification facilitates the utilization of the same view config file across different rviz instances representing robot. rviz view can also be started from command line for a particular robot. Disable Groot Monitoring To avoid conflicts and ensure the successful operation of multiple nav2 stack instances, it is necessary to disable the groot monitoring feature in the nav2_param.xml file. Failing to do so would result in the subsequent instances encountering a port already in use by the first instance. Therefore, for each new nav2 behavior tree instance, either disable the groot monitoring or give a new port number to each nav2 stack initiation to prevent conflicts. This tutorial can be consider as starting guide to setup multiple robot in Gazebo simulation environment. We explored step-by-step process of setting up and controlling multiple Turtlebot3 robots using the Navigation2 (Nav2) stack. With each robot assigned its own namespace, individual control and navigation can be achieved through RViz2 or the command-line interface. This tutorial enables users to initiate customizable multi-robot scenarios, implement scan topic remapping for proper connectivity, and optimize rviz view files for seamless configuration. By following these instructions, users can create and control multiple Turtlebot3 robots, opening up possibilities for multi-robot simulations and advancing robotics research. -- -- 3 As the technical lead at Intel Corporation, my work orbits around the intriguing world of Robotics and Artificial Intelligence. Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@arshad.mehmood/a-guide-to-multi-robot-navigation-utilizing-turtlebot3-and-nav2-cd24f96d19c6"
        }
    },
    {
        "title": "Navigation (ROS 1) Vs Navigation 2 (ROS 2)",
        "content": "Sign up Sign in Sign up Sign in Sharad Maheshwari Follow -- Listen Share Pre-requisites \u2014 An overview of ROS 1 and 2, Managed Nodes in ROS 2 and Behavior Trees. Aaand curiosity!Check out my robotics youtube channel here for videos on all of these :) Robotics is evolving with a shift from ROS 1 to ROS 2, and so is Robot Navigation. In this post, we talk about the why and how of this transition and the differences between the Navigation Stack in ROS 1 and the Navigation Framework in ROS 2. We will go deeper into the design of both to understand how they\u2019re different. We don\u2019t want to understand it just superficially, do we? \ud83d\ude03 To understand the need and design of Navigation 2 in ROS 2 (here onwards called nav2), we start with Navigation in ROS 1 (here onwards called nav stack) followed by its shortcomings. We then talk about how Navigation 2 is better, stronger and faster \ud83d\ude03 This post covers the following \u2014 1. What is the Navigation stack in ROS 1?2. What does it look like (design)?3. What were its major shortcomings?4. What is Navigation 2 framework in ROS 2?5. What does it look like (design), thus solving many past issues with the nav stack Let\u2019s begin. Hold on to your pants! The nav stack is a collection of packages running together in harmony in ROS 1 for mobile robot navigation. It takes information from odometry, sensors streams and goal pose commands and outputs safe velocity commands to our mobile robot\u2019s base. Duh! You probably knew this already, eh? So let\u2019s dig deeper into its design. The nav stack has multiple packages running together. And it largely works around the move base package \u2014 the central brain of our navigation task with all state machines, planners, internal cost maps and recovery behaviors. In the design shown above, \u2014 white nodes are required components that are already implemented \u2014 blue nodes are required and need setup for each robot platform \u2014 gray nodes are already implemented and optional Instead of directly talking about what\u2019s inside the brain (move base \u2014 the white node), we first treat it as a black box and break it down later. Let\u2019s first talk about what this black box needs. 1. odometry source\u2014 needs odometry information as nav_msgs/Odometry to know about the robot\u2019s pose and motion2. sensor sources \u2014 needs laser scan or point cloud information of the environment for obstacle avoidance during local and global path planning (using cost maps)3. sensor transforms \u2014 needs the robot to be publishing transforms between coordinate frames. This includes odom->base_link transform form odometry (and map->odom transform if a map is used)4. base controller \u2014 needs a robot base controller which accepts the output of move base package as geometry_msgs/Twist velocity command to the robot5. (optional) amcl \u2014 needs for map->odom transform when using a map6. (optional) map server \u2014 needs for loading a map when in use So, this is what our central brain needs. Oh yes, it also needs goal position (geometry_msgs/PoseStamped ) as an external trigger. How else would it know the goal position of the robot, eh? Note \u2014 The navigation system can be intialised with or without a static map. Without it, the robot only considers encountered obstacles and makes optimistic global plans about unseen areas. It re-plans the path later when more obstacles are encountered. Essentially, move base takes all this information, plans global and local paths considering global and local cost maps respectively and spits out velocity commands until the purpose is served. Let\u2019s finally look at the components inside move base \u2014 1. global costmap \u2014 a cost map (voxel grid\u2019s 2D representation) of the entire map for global/full-length path planning. It is also augmented with real-time obstacle information from sensors2. local costmap \u2014 a cost map of the robot\u2019s local (visible) region for local/short-distance path planning. This cost map is built only with real-time obstacle information from sensors3. global planner \u2014 planner using A* algorithm for current pose to end goal pose path planning. It is used only as a high-level guide for navigation in an environment. It takes in global costmap, robot localisation information and the goal pose4. local planner \u2014 planner using Dynamic Window Approact(DWA) for short-distance path planning. It is used for actual velocity commands to the robot. It takes in local costmap and global planner\u2019s path as a high-level guide5. recovery \u2014 behaviors used when the robot is stuck (potential failure) In summary, global and local planners use odometry, sensor information, costmaps, and goal position to spit out velocity commands to the robot. With all of this, the nav stack became a force to reckon with in applied mobile robotics. In fact, with its efficient Voxel-based 3D mapping (modeling unknown 3D space), it beautifully solved the problem of constructing, updating and accessing high-precision 3D perception data. Sounds like the perfect dream, right? Well. Problems with the Navigation Stack in ROS 1 \u2014 The world isn\u2019t perfect. Let\u2019s see. ROS 2 was built to break ROS \u201cout of the lab\u201d. Navigation 2 builds on ROS 2 and the successful legacy of the nav stack. Adding to ROS 2 enabled reliability, security and speed, Navigation 2 aimed to solve the mentioned problems in the nav stack Hold on. That\u2019s a lot of jargon at once. Let\u2019s gently get into the details of Navigation 2 in ROS 2 There are two important design patterns in Nav2 \u2014 Additionally \u2014 And here\u2019s what the design looks like \u2014 Right off the bat, the new brain (blue box) needs Behavior Tree Design (remember, no state machine, but BT?) and transforms/map/sensor data like before. Now, let\u2019s look at the core of this design. This is the highest-level component and entry point, hosting the Behavior Tree to implement navigation behaviors. Once it gets a goal pose from the user, it orchestrates the navigation task with its Behavior Tree. ROS 2 action servers are used to communicate with BT navigator through a NavigateToPose action message to request for navigation. In turn, the Behavior Tree inside also uses subsequent action servers in controller, planner, behavior, smoother servers to control efforts, computer plans, perform recoveries, etc. BT navigator\u2019s Behavior Tree nodes communicate with modular servers (controller, planner, behavior, smoother, custom) to orchestrate and navigate the robot. Each action server will have its own unique .action type for interaction. These servers host multiple relevant algorithm plugins for their tasks and one of them can be chosen by the Behavior Tree Node at runtime. Furthermore, they implement a standard plugin interface to allow for new algorithms to be created and selected t run-time. Talking about giving power to the developer, eh? \ud83d\ude01 All the servers shown in the design are implemented as ROS 2 Managed (Lifecycle) Nodes. Lifecycle Manager coordinates the program lifecycle of BT Navigator and subsequent server Nodes. It will step each server through the managed node lifecycle: inactive, active, and finalized. This is pretty neat, don\u2019t you think? And kids, this is how nav2 one-upped nav stack and became the new cool thang! A recap with some bonus info! This post was an overview of Navigation in ROS 1 and 2. If you wish to go deeper, please check out their open-source code/support and papers below. -- -- Building Robots Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@thehummingbird/navigation-ros-1-vs-navigation-2-ros-2-12398b64cd"
        }
    },
    {
        "title": "ROS2 Gazebo World 2D/3D Map Generator",
        "content": "Sign up Sign in Sign up Sign in Arshad Mehmood Follow -- Listen Share For realistic navigation utilizing Nav2, maps are typically produced employing SLAM methodologies, which extends to simulations in Gazebo as well. This ensures the incorporation of SLAM-generated maps in the path planning process, thereby facilitating a more accurate verification and behaviour in simulated environments. However, there are instances where the primary focus lies solely on the Nav2 path planning module. In such scenarios, the requirement shifts towards synthetic, yet precise, maps that can be utilized for development, testing in conjunction with simulations, and at times, actual deployment too. Creating maps for ROS2 Navigation 2 (Nav2) in Gazebo has traditionally been a challenging task. The prevailing method involved employing Simultaneous Localization and Mapping (SLAM) and navigating robots through the environment to create these maps. For ROS1 Kinetic, a plugin was developed to mitigate this issue, enabling map creation without relying on SLAM. However, this solution was limited to ROS1 and generated 2D maps only. This article presents a Gazebo 11 (Classic ) plugin, interfaced with a ROS2 service, specifically develop to generate 2D (.pgm) and 3D maps (Point Cloud) directly from the Gazebo world. The ROS2 Gazebo map generation plugin addresses these issues and provides several new enhancements. Here are the key features of the new plugin: The fundamental principle employed here involves partitioning the map space into uniformly distanced points. Each of these points is then inspected to ascertain whether a collision is likely to occur in its vicinity. Map Output Samples Cloning and Building In order to utilize this plugin, a Python script is included that dispatches the service request. The script anticipates the following parameters: Usage Assuming GAZEBO_MODEL_PATH env is properly setup and gazebo_map_creator plugin install is sourced into environment Console A Console B Viewing output files Once the pcl GUI is up, use 1,2,3 & 4 keys to change point cloud color for better viewing. Corners (-c option) To define the area for map generation, it\u2019s sufficient to provide just two points of a 3D box \u2014 the bottom right and the top left. These two points establish the rectangular boundary within which the map will be generated. The bottom right corner is determined by the minimum values of the X, Y, and Z coordinates, while the top left corner is associated with their maximum values. These two sets of coordinates solely serve the purpose of enabling the scanning process for map creation. Resolution (-r option) The entire space, targeted for map generation, is partitioned into a series of equidistant points. This partitioning process is managed by the \u2018-r\u2019 parameter, which sets the level of segmentation within the given area. For instance, with an area length of 6 units, utilizing an \u2018-r\u2019 parameter value of 0.01 would yield a division into 600 uniformly spaced points across the defined area. The default value of 0.01 will result in 100 It\u2019s important to note that the segmentation directly influences the resolution of the resultant map. A higher number of divisions or \u2018units\u2019 correlates to a higher-resolution output. For instance, the default \u2018-r\u2019 parameter value of 0.01 implies that each unit of space (e.g., 1 Meter) is divided into 100 points, meaning these points are spaced 1 Centimeter apart. Thus, this parameter enables a more detailed representation of the environment, contributing to the accuracy and precision of the resulting map. Optimizing Map Generation ( \u2014 skip-vertical-scan) By default, the plugin generates both 2D and 3D maps. The scanning process, under regular circumstances, performs a comprehensive scan, considering every single point within the defined space. While this thorough method ensures a highly detailed output, it could be time-consuming, particularly when the user is solely interested in 2D maps. To cater to this specific need, an option \u2014 skip-vertical-scan has been introduced. When this option is activated, the scanning process omits the full vertical scan, executing only a single horizontal scan at the bottom z value. This alteration results in a significant reduction in processing time, delivering results more swiftly. In this mode, a collision is detected by performing a single pass in the horizontal direction at the lowest z value, complemented by a vertical scan up to the desired top height. This ensures that potential collisions are effectively identified, even in the expedited scanning mode. Height consideration for 2D map In the context of 2D map generation, it\u2019s crucial to ensure that the defined rectangular area encompasses the operational height of the robot, including any potential payload. The bottom limit of this area should be slightly above the height of the floor, or slightly raised from the ground, accommodating for situations where the floor level might be somewhat elevated from the ground surface otherwise the collision detection logic will detect all area occupied due to floor contact. Collision check multiplier (-d option) The -d (default 0.55)distance parameter is introduced to customize the extent to which the plugin should look for a horizontal collision. This parameter directly influences the granularity of the mapping process. In scenarios where lower resolution is used, a larger multiplier value is preferred to ensure a broader area is checked for potential collisions. Conversely, for higher resolution mapping, a smaller distance value is more suitable as it provides finer and more precise results. This functionality allows users to tailor the map generation process to their specific needs, be it for broad, low-resolution mappings or more detailed, high-resolution mappings. e.g 0.5 value will look for collision detection up to half way of next point. The other half is owned and should be detected by next point. The default 0.55 will allow the detection to go slightly beyond into next point\u2019s territory. The improves overall detection process. Threshold (-t option) Tthe -t (default 255)threshold parameter is used to denote the filled area in the .pgm file. Specifically, the plugin calculates the value as (255 - threshold) to mark the occupied areas in the map. This is due to the fact that in the grayscale .pgm file, a pixel value of 0 represents an occupied or filled area, while a pixel value of 255 denotes an unoccupied or free space. This intuitive color mapping enhances the visual interpretation of the generated maps, making them easier to understand and use in subsequent navigation tasks. The article introduces a ROS2 Gazebo plugin designed to streamline the creation of 2D and 3D maps from Gazebo world simulations. This plugin supports advanced features such as ROS2 service-based call system, horizontal collision check, YAML file generation, and a conversion to a system plugin. With this plugin, maps are generated by partitioning the space into equidistant points and assessing potential collisions. This process is customizable, with parameters to define the scan area, resolution, collision check multiplier, and pixel threshold value for the maps. The plugin offers a significant advancement for generating maps in Gazebo, particularly for applications focused on Nav2 path planning where precise synthetic maps are required for development, testing, and deployment. It also includes optimizations such as the option to skip vertical scans, resulting in faster result generation when only 2D maps are desired. Source code Acknowledgement: Gazebo Custom Messages You can connect with me on LinkedIn -- -- As the technical lead at Intel Corporation, my work orbits around the intriguing world of Robotics and Artificial Intelligence. Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@arshad.mehmood/ros2-gazebo-world-map-generator-a103b510a7e5"
        }
    },
    {
        "title": "No Title Found",
        "content": "Sign up Sign in Sign up Sign in Santosh Balaji Selvaraj Follow -- Listen Share Please note the source code for this tutorial series can be found at this link The series of articles focus on how a robot arm can be controlled using MoveIt2 framework. When it comes to manipulation the following are the components required for its control. In the later part of tutorial series I will be explaining about the components listed above.Enough of theory for now. Lets dive into making our first robot arm model and try to visualize it !!!! The first step is to create an URDF representing the Robot model. For this tutorial I have considered using robot model with 4 degrees of freedom. The following configuration is to enable usage of ros2_control for the above described URDF joints. As mentioned before joints if it is not fixed then a control system is required to manage its actuation. It can be a closed loop or an open loop based control system. In order to enable this ros2_control provides with different control strategies. For our current use case we have taken 4 joints with revolute type (rotates across x or y or z \u2014 roll or pitch or yaw axis). The following launch file starts the below mentioned nodes The following are the steps required to build and launch the simulation Run the following command in your ROS workspace to build the description package colcon build \u2014 packages-select pick_and_place_description Run the following command to start gazebo simulation ros2 launch pick_and_place_description start_gazebo.launch.py Some rights reserved  -- -- I am from planet earth dreaming of sending robots to outer space Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@santoshbalaji/manipulation-with-moveit2-visualizing-robot-arm-in-simulation-1-8cd3a46d42b4"
        }
    },
    {
        "title": "ROS MoveIt! \u2014 All You Need to Know To Start",
        "content": "Sign up Sign in Sign up Sign in Ricardo Tellez Follow -- Listen Share I remember when we participated at thE Robocup At Home competition in 2013 with the Pal Roboticsteam (actually that project was the germen of current The Construct company). At that competition, we used a humanoid robot to do home tasks like bringing stuff to the owner, detecting emergency situations or just following its owner around. The most complicated of all the tasks was to bring something to the owner. In order for a robot to bring you something, a lot of systems are required to be put in place: Exactly! At that time (2013) ROS did not have any package that dealt with the grasping problem. Luckily, today we have the MoveIt! ROS packages that handle that procedure\u2026 and they do it quite well! The basic task of the MoveIt! system is to provide the necessary trajectories for the arm of a robot to put the end effector in a given place. What does that mean? When we want to grasp something with a robot arm, we need to move all the joints of the arm so that the final part of the arm, the one that has the gripper or hand (which is called the end effector), can be at the proper location of to grasp the object. Moving the arm to achieve that position is a non trivial task because you have to produce the sequence of values that every joint of the arm must follow (in coordination with the other joints), so the end effector moves from its current place to the desired place. This task is called motion planning. The result of a motion planning is the sequence of movements that all the joints of the arm have to perform in order to move from current location to the desired one. The following video greatly describes what is it all involved in the making of a motion planning: How to solve the problem of calculating the motion plan is a VERY DIFFICULT problem. Fortunately for us, that is what MoveIt! is an expert at. MoveIt! provides us the plan that the joints have to follow to move an arm from one position to another. In the following gif animation, you can see how a plan for making Fetch extend its arm is provided by MoveIt! MoveIt! provides this animation as a way of showing you how the planned trajectory will look like when executed by the robot(the animation is provided by the MoveIt! control page, more latter). Once we have that plan, it is only a matter of sending it properly to the ROS controlled joints. Fortunately, MoveIt! can also take charge of executing the plan in the robot (it is up to you if you want it just to provide you the plan, or plan+execution). In this gif animation, we show you how the simulated robot executes the plan created by MoveIt! on the previous step (you can make MoveIt! send the trajectory to either the simulated or real robot). The code that generates the motion plan is called the planner. Being motion planning a difficult problem, we cannot consider that it has been solved. There is not the best solution to find the joints movement for any robot arm. Because of that, there exists different approaches (different planners) to the calculation of the plans. Move provides off-the-shelf several planners, each one with its advantages and drawbacks. You can read the full list of provided planners here. Additionally to that list, you can develop your own planner too and provide it to MoveIt! as a plugin. You can learn more about MoveIt! plugins here. It is quite a subject, so in case you would like us to do a tutorial on MoveIt! plugins, just let us know below in the comments of the post. In order to be able to use MoveIt! with your robot, first, you have to configure it. Fortunately, configuration of a robotics arm is very simple, provided that you have a URDF file describing your robot. That is basic requirement. You need to specify someway how your robot is composed, length of the different parts of the arm, which joints it contains, among other things. If you don\u2019t know how to create the URDF of your robot, take this online course to learn how to do it. Armed with your URDF, you only have to launch the MoveIt! assistant. That is a beautiful piece of configuration that simplifies A LOT the setup of your arm robot (I remember the old days\u2026). Assuming that you have MoveIt! installed in your system (very simple step, watch this page to learn how to install it), now you can launch the assistant with the following command: > roslaunch moveit_setup_assistant setup_assistant.launch Let\u2019s do now a simple example of configuration. Let\u2019s configure the robot of the figure. If you do not have that robot to practice with, you can use the simulation of it. Watch this video to learn how to have that simulation running in your computer in only 5 minutes (really, that quick). For the rest of the tutorial, we are going to assume that you either have the ROS packages of that robot installed in your computer (watch installation tutorial here), or that you are using the simulation as explained in the previous paragraph. We highly recommend you to do the simulation path!! It is dangerous to do experiments with the real robot when you still don\u2019t know how it works. Now you are ready to make MoveIt! create a plan for that robot. Remember that MoveIt! has two separated functions: create the plan and send the plan to the robot. Here we deal with the first. On the next section below you will learn how to send it to the robot. > roslaunch myrobot_moveit_config demo.launch Now that you have a plan for the robot, and that the plan is visualized in the planner window, it is time to send the plan to the robot. The instructions for getting the simulated (or real) robot executing the trajectory are the following: controller_list:- name: sia10f/joint_trajectory_controlleraction_ns: \"follow_joint_trajectory\"type: FollowJointTrajectoryjoints: [joint_s, joint_l, joint_e, joint_u, joint_r, joint_b, joint_t] Basically, here you are defining the Action Server (and the type of message that it will use) that you will use for controlling the joints of your robot. controller_joint_names: [joint_s, joint_l, joint_e, joint_u, joint_r, joint_b, joint_t] <launch><rosparam file=\"$(find myrobot_moveit_config)/config/controllers.yaml\"/><param name=\"use_controller_manager\" value=\"false\"/><param name=\"trajectory_execution/execution_duration_monitoring\" value=\"false\"/><param name=\"moveit_controller_manager\" value=\"moveit_simple_controller_manager/MoveItSimpleControllerManager\"/></launch> <launch> <rosparam command=\"load\" file=\"$(find myrobot_moveit_config)/config/joint_names.yaml\"/> <include file=\"$(find myrobot_moveit_config)/launch/planning_context.launch\" > <arg name=\"load_robot_description\" value=\"true\" /> </include> <node name=\"joint_state_publisher\" pkg=\"joint_state_publisher\" type=\"joint_state_publisher\"> <param name=\"/use_gui\" value=\"false\"/> <rosparam param=\"/source_list\">[/sia10f/joint_states]</rosparam> </node> <include file=\"$(find myrobot_moveit_config)/launch/move_group.launch\"> <arg name=\"publish_monitored_planning_scene\" value=\"true\" /> </include> <include file=\"$(find myrobot_moveit_config)/launch/moveit_rviz.launch\"> <arg name=\"config\" value=\"true\"/> </include> </launch> Based on the knowledge above, I invite you to experiment with the other options that the MoveIt! Rviz tools has and try to generate the specific poses that we configured during the assistant launch. Up to this point, you have used MoveIt! with a robot that is isolated from the world. This means that the robot cannot collide with anything but itself. However that is not the real situation of most of the robots. We want robots to be able to plan in an environment where other objects are around. MoveIt! allows to introduce known obstacles in a prior step to planning, so the planning algorithm can take it into account and do not generate trajectories that would collide with the objects around. In the previous figure, MoveIt! is taking into account the table that it is detecting with a Kinect sensor. The inclusion of obstacles in the trajectory planning is something delicated that cannot be described here now, specially if you want to embed that into a full grasping pipeline, where the robot has to do actual grasps of objects (that at the end, that is what we are making all this for). In case you are interested in learning how to include obstacles in the planning trajectories, and how to actually grasp the objects, please take a look at this course that teaches that subject. Of course, one of the first applications to practical things of MoveIt! is to industrial robots. Industrial robots are based on path planning. Basically, that is the only thing they are required to do. So when MoveIt! came out, the first application was for that kind of robots. This has lead to he creation of the ROS Industrial consortium, a group of people and institutions that try to spread ROS into the industrial sector. Remember that at present ROS is more focused into service robots for which there is no actual industry or market yet. The only robotics market that actually exists is the industrial one. So the move makes perfect sense, since the industrial robotics market is the one closer to provide some revenue. Under the ROS Industrial umbrella, the consortium makes extensive use of MoveIt! for the control of already existing comercial robots like Kuka, Universal Robots, Motoman, ABB or Fanuc robots. The ROS Industrial packages provide, not only the connection to MoveIt! but also the drivers to ROSify those robots. That means that, by using the ROS Insdustrial packages you can control those commercial robots with ROS and with MoveIt! You can download the ROS Industrial packages from their github. You can have a good practical introduction to ROS Industrial with this course. MoveIt! is so great package for finding trajectories that can also be used for generating 3D trajectories for other systems that are not robotic arms, like for example for drones. Since drones have to move in a 3D environment while avoiding the obstacles in its way, having a generated plan by MoveIt! makes it a lot easier. For instance, this course teaches how to make a drone navigate using MoveIt! Next animation shows how to use MoveIt! for generating a simple trajectory of a drone. Related content: [ROS Q&A] 108 \u2014 Visualize inflation layer in RViz MoveIt! package is the way to go to control robotic arms (either industrial or humanoid robot arms). With its plugin interface for planners, you can attach your own planner if you don\u2019t like what already provided off-the-shelf, and keep using the ROS infrastructure. It is without doubt, the fastest way to use your robotics manipulator. Give it a try and let us know what your experience has been. To learn more about MoveIt! check the following tutorials: Now, what I would like to hear is what do you think of MoveIt! Answer the following questions below in the comments: Originally published at www.theconstructsim.com on January 25, 2018. -- -- Teacher of ROS @ Master of Industry 4.0 | CEO @ The Construct Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@rtellez/ros-moveit-all-you-need-to-know-to-start-f9daa896eff6"
        }
    },
    {
        "title": "How to Control the UmiRobot Using ROS 2, micro-ROS, and MoveIt: A Step-by-Step Guide.",
        "content": "Sign up Sign in Sign up Sign in Baris Yazici Follow -- Listen Share In the past year, there has been a huge interest in low-cost robotic manipulators, driven by their potential to automate household tasks using advances in Imitation Learning like the Aloha robot. These affordable robots offer several key advantages: Such traits make these robots particularly appealing for robotics enthusiasts and students, providing a practical platform to test and refine learning algorithms. In this context, 3D printed manipulators like the UmiRobot, introduced by Murilo M. Marinho, Ph.D., serve as a stepping stone for students to learn ROS and experiment with algorithms on actual hardware. In this article, I\u2019ll discuss how I\u2019ve worked on integrating the UmiRobot with ROS 2. This integration aims to simplify UmiRobot control, significantly lowering the barriers for users. The following video shows the UmiRobot in action, executing motion plans generated by MoveIt. https://www.youtube.com/watch?v=zb3xAI4kboM I adapted the UmiRobot for ROS 2 integration with MoveIt by modifying the original design proposed by Marinho et al. The core of these enhancements is the ESP32 microcontroller, equipped with a dual-core processor that efficiently handles multiple tasks. For programming ESP32, the ESP-IDF (IoT Development Framework) was used. It\u2019s an open-source SDK that facilitates firmware compilation and flashing. ESP32 is also supported by the micro-ROS. For more details on starting with ESP-IDF, visit the ESP-IDF development guide. The diagram below demonstrates how the ESP32 connects the 4 servo motors and the user\u2019s computer which runs ROS 2 and sends hardware commands. Servo 1 is connected to the second GPIO, Servo 2 to the fourth GPIO, Servo 3 to GPIO 17, and Servo 4 to GPIO 18. The integration of ROS 2 with the UmiRobot utilizes micro-ROS on middleware level. Micro-ros is ROS 2 adaptations for microcontrollers. By using micro-ROS we eliminate the need for separate hardware drivers, enabling direct communication with the hardware through ROS 2 topics. I use a ROS 2 topic, /set_servo_angle, to maintain the communication between the ROS 2 hardware interface and the microcontroller. The micro-ROS example subscribes to this topic, receiving angle commands for each motor, which are then executed using the MCPWM library to send the appropriate PWM signals to the servos. To enhance functionality, the integration with MoveIt allows for trajectory planning and execution while avoiding collisions. This requires a MoveIt configuration package and a URDF file with collision meshes for collision detection and inverse kinematics calculations. The entire codebase, including the integration of ROS 2 with the UmiRobot is open-sourced. Key components of the project are available on my GitHub repositories. The servo drive example, part of the micro_ros_espidf_component repository, can be explored here: Servo Drive Example. Additionally, the ROS 2 hardware interface package, which includes umi_robot_msgs, and the MoveIt configuration package, are both available in the umi_robot_ros2 repository, accessible at: UmiRobot ROS 2 Repository. -- -- Software engineer based in Munich. Interested in reinforcement learning in robotics application. Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@barisyazici/how-to-control-the-umirobot-using-ros-2-micro-ros-and-moveit-a-step-by-step-guide-f1de1ac3878e"
        }
    },
    {
        "title": "Integrating NVIDIA Isaac Sim and ROS 2 Humble with MoveIt for Advanced Robot Interfaces",
        "content": "Sign up Sign in Sign up Sign in Kabilankb Follow -- Listen Share In the rapidly evolving field of robotics, the ability to simulate and control robots efficiently is crucial for development and testing. NVIDIA Isaac Sim and ROS 2 (Robot Operating System 2) are two powerful tools that have significantly advanced these capabilities. When combined with MoveIt, a widely-used framework for robot motion planning, they provide a robust solution for creating sophisticated robot interfaces. This blog explores how to integrate NVIDIA Isaac Sim with ROS 2 Humble and MoveIt to build and simulate advanced robotic systems. The `joint_state_publisher` in ROS Humble is a node that publishes the state of all robot joints (positions, velocities, and efforts) based on either user input via a GUI or configuration files. It is commonly used for simulating and visualizing robot states in a robotics application. This helps in testing and debugging the robot\u2019s kinematics without needing the physical hardware. In Isaac Sim, UR robot subscribes to the `joint_state_publisher` topic to receive joint velocity and position information, enabling accurate simulation and visualization of its movements based on the published joint states. Using the Joint State Publisher, joint commands are sent to the robot, allowing it to move its joints accordingly. In Isaac Sim, create an Action Graph to define the robot\u2019s behavior and interactions. Here\u2019s a simplified example of what this might involve: Add nodes to the Action Graph for publishing and subscribing to ROS topics relevant to MoveIt. This might include topics for joint states, motion planning requests, and trajectory execution. Create nodes to handle motion planning requests and execute planned trajectories. This involves: If using MoveIt services for planning and execution, add service client nodes to call these services and handle their responses. MoveIT will manage the robot\u2019s joint operations within Isaac Sim, utilizing the trajectory data it generates to ensure precise and coordinated movements. This integration allows for seamless control and execution of complex robotic tasks. By leveraging MoveIT\u2019s advanced planning capabilities, the system can dynamically adapt to changes and optimize performance. The collaboration between MoveIT and Isaac Sim enhances the robot\u2019s efficiency and accuracy in a simulated environment. This setup ensures a robust framework for testing and development in robotic applications. Integrating MoveIt with NVIDIA Isaac Sim using the Action Graph allows for advanced robot motion planning and execution in a simulated environment. This setup provides a powerful tool for developing and testing robotic applications before deploying them to real-world robots. -- -- nvidia ambassador Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@kabilankb2003/integrating-nvidia-isaac-sim-and-ros-2-humble-with-moveit-for-advanced-robot-interfaces-0fe9a16728db"
        }
    },
    {
        "title": "Enabling Multi-Robot ARM in Gazebo for ROS2",
        "content": "Sign up Sign in Sign up Sign in Arshad Mehmood Follow -- 1 Listen Share In recent years, the simulation of robotic systems has gained prominence due to the possibilities it offers in testing and development. Among the simulation environments available, Gazebo stands out for its robustness and flexibility, being widely used in conjunction with the Robot Operating System (ROS). However, a capability that has yet to be thoroughly explored within the ROS2 ecosystem is the spawning of multiple robotic arms within the Gazebo simulation environment. This article aims to bridge that gap, demonstrating a scalable and flexible approach to spawn multiple UR5 robotic arms in Gazebo using ROS2. This tutorial presents an ROS2 package that demonstrates the simultaneous spawning of multiple robotic arms in a Gazebo simulation. Historically, such an application is difficult to come by within the ROS2 community, leaving a knowledge gap for developers looking to implement multi-arm robotic systems. The work outlined here not only fills this void but also offers a practical guide for beginners and interested parties looking to replicate similar multi-robot simulations in Gazebo. The UR5 robotic arm, a versatile and widely-used model in robotics, serves as the core of this demonstration. By focusing on this particular model, the tutorial ensures a broad relevance to a multitude of potential robotic applications. The proposed solution emphasizes scalability and flexibility, primarily through robot configuration defined in the launch file as a list. This design choice allows users to easily modify the file to add or remove robot instances. As a result, the approach lends itself well to future enhancements, such as reading robot information from external files like JSON. In the given launch file, four robots are added as a baseline. A key feature of this approach is that it negates the need to modify the URDF file for each robot to add its namespace. Instead, the robot description file is generated dynamically. This simplifies the setup process and makes it easier to manage multiple robot instances. An important issue that arises when working with multiple robot instances is namespace management. In this solution, robots are spawned sequentially to circumvent this problem. This is crucial because the ROS2 control updates the process global namespace for each instantiation. As such, all robot initialization procedures, including the broadcast controller and joint trajectory controller for a given instance, must be fully operational before triggering the next robot spawn. This sequential execution is achieved through the Launch RegisterEventHandler mechanism offered by ROS2. To send movement commands to MoveIt2, the Pymoveit2 library is employed. This library is modified in the fork with several bug fixes to ensure smooth operation. Using Pymoveit2, client programs must run with a robot namespace to connect to a specific robot instance. This allows for efficient management of multiple robot instances and clear communication pathways between the components of the system. However, in recent times, MoveIt2 has introduced native Python bindings in its codebase. These bindings enable direct interaction with MoveIt2 via Python, eliminating the need for additional libraries like pymoveit2. This advancement simplifies the setup process, reduces dependency issues, and potentially enhances performance and stability. As part of ongoing improvements and in the spirit of keeping up with these updates, future versions of this tutorial may transition to using the Python bindings provided directly by MoveIt2. This would replace the current reliance on the pymoveit2 fork, streamlining the implementation process and ensuring compatibility with future developments in MoveIt2. Consequently, this change will not only refine the tutorial but also make it more adaptable and robust for future use-cases. For further details on pymoveit2 visit https://github.com/AndrejOrsula/pymoveit2 Moveit_py: https://moveit.picknik.ai/main/doc/examples/motion_planning_python_api/motion_planning_python_api_tutorial.html Moveit2: Cartesian Path vs Kinematic Path The terms \u201cCartesian path\u201d and \u201ckinematic path\u201d are related to motion planning in robotics, and they represent two distinct ways of specifying the robot\u2019s movement. Cartesian Path: The Cartesian path describes the movement of a robot (particularly the robot\u2019s end-effector or tool) in Cartesian coordinates, that is, in terms of x, y, and z coordinates in a three-dimensional space. When you specify a Cartesian path for a robot, you\u2019re essentially providing a series of points in space that the robot\u2019s end-effector must move through. This approach is often useful when the precise path in space is critical, such as when the robot is painting or welding. In these scenarios, the tool needs to follow a very specific route in Cartesian space. However, planning in Cartesian space can be computationally complex because the robot must solve the inverse kinematics problem to find the necessary joint angles to achieve the desired position and orientation in space. Kinematic Path: Kinematic path, or joint-space planning, on the other hand, involves planning the movement of the robot in terms of its joint parameters, such as joint angles for revolute joints or joint displacements for prismatic joints. In this approach, you\u2019re primarily concerned with moving the robot from one configuration (a specific set of joint parameters) to another. The exact path that the end-effector takes in Cartesian space may not be as important in kinematic path planning. The advantage of this approach is that it can be computationally simpler than Cartesian planning because it operates directly in the space of the robot\u2019s joints. However, it might not be suitable when the precise path in Cartesian space matters. In summary, the choice between Cartesian path planning and kinematic path planning depends on the specific requirements of the task that the robot needs to perform. Building upon the outlined procedure for simultaneous multi-robot arm spawning in Gazebo, it is equally important to address the potential issues associated with environmental interactions, specifically, collisions with the ground plane. To ensure a stable environment for the UR5 robotic arms to operate, the example includes the incorporation of a ground plane. This fundamental element of the simulation serves as a foundational support for the robotic arms, and its presence is crucial for their correct operation and the accurate interpretation of their movements. However, simply adding a ground plane may inadvertently lead to unwanted collision events. These could compromise the operation of the robotic arms, leading to inaccuracies in motion and potential simulation errors. To prevent this, appropriate measures are taken in the simulation design to ensure that the robotic arms are spawned in such a way that they are correctly situated above the ground plane, thereby avoiding direct collisions with the floor. The spawning parameters, including the position and orientation of each robot, are chosen to ensure a safe distance from the ground plane at all times. This not only enhances the stability and reliability of the simulation but also mimics the real-world scenarios where robotic arms are typically mounted at a certain height from the floor. This detailed attention to the simulation environment, specifically the careful integration of a ground plane, significantly contributes to the realism and practical utility of this tutorial, making it a comprehensive guide for simulating multiple robotic arms in Gazebo. The package is verified with ROS2 Foxy and Humble. Dependencies: ROS2 [Foxy or Humble], moveit2, Gazebo 11 Clone and Build Console A (Launch Gazebo) Console B (Trigger ARM movement) ARM1 movement to position [0.5, 0.4,0.2] using kinematic path planner ARM4 movement to position [0.5, 0.4,0.2] using cartesian path planner __ns:=/namespace parameter is to direct commands to a specific instance of robot. Position coordinates are given relative to arm position. In summary, this ROS2 package provides a scalable and flexible way to simulate multiple UR5 robotic arms in Gazebo. It paves the way for developers to build complex multi-robot systems and test them in a simulated environment, accelerating the development of advanced robotics applications. I hope that this tutorial will serve as a useful guide for those seeking to venture into the world of multi-robot simulations with Gazebo and ROS2. Source Code Location: https://github.com/arshadlab/multi_robot_arm/tree/master -- -- 1 As the technical lead at Intel Corporation, my work orbits around the intriguing world of Robotics and Artificial Intelligence. Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@arshad.mehmood/enabling-multi-robot-arm-in-gazebo-for-ros2-dc18981c03c6"
        }
    },
    {
        "title": "For ROS2 Beginners \u2014 Install ROS 2 and Gazebo on Ubuntu 22.04 Jammy (LTS) Step-by-Step Instruction",
        "content": "Sign up Sign in Sign up Sign in Claudia Yao Follow -- Listen Share Recently I need to install ROS 2 to set up a robot simulation environment. However, I have never used Linux operation system nor ROS 2. Initially, I tried to set up a virtual Ubuntu 24.04 environment on Cisco VirtualBox, but it runs super slow and I could not endure it any more. Since I have an idle computer, I plan to erase the original content and install Ubuntu directly. First, be cautious to install the latest Ubuntu 24.04 desktop. Although it is a long-term support (LTS) version, it is not compatible with the stable ROS 2 and Gazebo versions. At the time of writing, my recommendation is that new users install: Ubuntu Jammy 22.04 ROS 2 Humble Hawksbill Gazebo Fortress (Gazebo Harmonic might also be OK) If your computer has AMD processor, just open this page to install 64-bit PC(AMD64) desktop image (https://releases.ubuntu.com/jammy/) However, if your computer has ARM processor, you might need to refer to the steps mentioned in this StackExchange post. Open this link to prepare for a Bootable USB Stick. Just follow the steps mentioned on the page to finish the whole installation process. Download ROS 2 Humble Open the page: https://github.com/ros2/ros2/releases. Choose one file from Assets list which suits your computer hardware configuration and Ubuntu version. For me, I use the ros2-humble-20240807-linux-jammy-amd64.tar.bz2 version. Install ROS 2 Follow the tutorial on the Documentation Page of Ros 2 Humble to install ROS 2. Please note that the page info is not up-to-date. In the following section Downloading ROS 2, the file name ros2-package-linux-x86_64.tar.bz2 does not exist. You need to use the downloaded file name in the above step. Other steps on this page are correct and you just need to follow them step by step. Please note that you do not need to run Environment Setup and following steps on this page. They are not up-to-date. After installing ROS 2, let\u2019s do environment setup. At Environment Setup page, we need to configure setup.bash, so that every time, you open Terminal window to execute commands, the ROS 2 environment has been ready. Most of the steps at Environment Setup page is up-to-date, except the following one. The setup.bash file is usually located under <ros2-installation-path>/ros2-linux/, instead of the path the following screen capture mentioned. Besides the above path issue of the setup.bash, you could follow the steps mentioned in the Environment Setup page. Brief description: Gazebo is an open source 3D Dynamic simulator used for robotics development. It offers a wide of sensors and interfaces as well as multiple physics engines. Based on the above combination table, I will install Gazebo Fortress (LTS) version. We could follow this documentation page to install it step by step. Once it is done, you could go to Tutorial of Gazebo and download the sample building_robot.sdf file, and then run command ign gazebo building_robot.sdf in Terminal to check if Gazebo simulation application could run on your computer correctly. Now your robot simulation environment is ready. You could follow ROS 2 documentation to learn tutorial and start wring your ROS 2 packages. Good luck! -- -- Founder of The Coding Fun - Python, Machine learning, LEGO robot coding, Scratch programming, Golang Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@claudia.yao2012/for-ros2-beginners-install-ros-2-and-gazebo-on-ubuntu-22-04-jammy-lts-step-by-step-instruction-08327c3764a5"
        }
    },
    {
        "title": "Robot Operating System: Controlling a Robot with the ROS Gazebo Plugins",
        "content": "Sign up Sign in Sign up Sign in Sebastian Follow Geek Culture -- 1 Listen Share In my recent articles about ROS and my project RADU, I showed how to launch a custom robot model in Gazebo and exposing its joints via special control nodes. These nodes accept commands to change their efforts, velocity, or position. However, the nodes are not translating the commands per-se and move your robot, you still need to write the code that interfaces with Gazebo. Gazebo plugins come bundled with a typical ROS installation. Plugins support several actuators, such as the differential drive or skid drive, and sensors, such as cameras, depth cameras and IMUs. The plugins communicate via controller nodes internally, and you do not need to expose them. Instead, they publish relevant topics such as the /cmd_vel to send linear and angular movement commands to your robot, or /odom which represents your robot's position since the simulation started. This article is a hands-on workshop on Gazebo plugins. You will learn how plugins function, and we will see how to use the diff-drive plugin in ROS2 to control a 4-wheeled robot. This article originally appeared at my blog admantium.com. During my robot project, I checked several other robot projects from the community. I was puzzled why some projects did not spawn any controller nodes, but still the above-mentioned topics /cmd_vel and /odom were published. My assumption was that the plugins for these robots would start the controllers automatically. Checking the official Gazebo documentation, we can find this paragraph: Previous versions of Gazebo utilized controllers. These behaved in much the same way as plugins but were statically compiled into Gazebo. Plugins are more flexible and allow users to pick and choose what functionality to include in their simulations. Source: official Gazebo documentation. A plugin is a C++ library following conventions as outlined in the plugin development guide. The plugins inherits from a base class \u2014 like sensor, system, visual or world \u2014 and expose the Load function. Here is an example from the plugin development guide. Once compiled, the plugin needs to be loaded via the robot\u2019s URDF model. When using a pre-made plugin, we just need to add it to the URDF model. Since the inception of ROS2, plugins have been steadily converted. The status page shows that individual plugins for e.g. cameras were combined into one package. Taking this as a reference, we can select and incorporate the plugins of our choice to make them workable with our prototype. We want to control how the robot moves. Four different steering principles are supported by plugins: differential drive, skid drive, Ackerman drive, and tri-wheel. My RADU robot is a 4-wheel vehicle and can be controlled as a skid-drive. Therefore, we will use the ros_diff_drive plugin. To use the plugin in addition to your robots\u2019 physical model is much less complex then adding the controller nodes manually. In fact, you just need to add a single <plugin> tag! Here is the complete example: The declaration consists of 4 configuration blocks. Tags in each block should be self-explanatory, so I will only briefly explain them. Once the plugin is available in your robot description, we can start Gazebo and the requires Nodes. We need to start three nodes explicitly \u2014 Gazebo, Joint State Controller, and the Robot State publisher \u2014 and then spawn the robot. The last step creates an additional controller node for the plugin. Here is the launch file: Launch it with ros2 launch radu_bot control.launch.py, and then we can see the available nodes: The last node, wheel_drive_controller, is subscribed to the topic cmd_vel with which we can control the robot. To control the robot, we run the teleop_twist_keyboard command, which translates directional keyboard presses into /cmd_vel messages. But when moving the robot, it behaved strange: The wheels were rotating along the y-axis! To fix this, we need to add explicit <axis> and <limit> tag to the joint. Then the robot moved smoothly. A long development journey comes to an end: Finally, the RADU robot simulation in Gazebo is modeled correctly and can move in a simulated world. To achieve this goal, we needed to learn about three distinct topics. First, which additional physical properties are needed for a Gazebo-compatible URDF model. Second, that we need additional control nodes in our ROS system that expose a robot\u2019s joints for effort, velocity or position manipulation and have these nodes translate cmd_vel commands for changes in the simulation. Third, plugins take the heavy lifting of the second part and only required a correctly configured specification and starting of addition join state/robot state joins. Building on this foundation, the robot can now be equipped with an image camera and a depth camera for simulation in Gazebo. -- -- 1 A new tech publication by Start it up (https://medium.com/swlh). IT Project Manager & Developer Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/geekculture/robot-operating-system-controlling-a-robot-with-the-ros-gazebo-plugins-e8055cf28f44"
        }
    },
    {
        "title": "Setting up Mujoco",
        "content": "Sign up Sign in Sign up Sign in Ganesh Prasanna Follow -- Listen Share Mujoco is a physics simulator developed by Roboti LLC, which is used in several different fields (ranging from animation to robotics) for simulating realistic scenarios. It is a very good alternative to Gazebo and VRep in the controls and motion planning community due to the fact that it is more precise (see paper) and better for simulating MPCs and multi body dynamics which need a very good mathematical approximation of the system. Recently Mujoco is being extensively used as a platform for benchmarking reinforcement learning algorithms after OpenAI gym released easy to use interface that can enable an agent to interact with a simulated environment. In this post I talk about how you can get this set up and ready to use with OpenAI\u2019s gym. The process of installing mujoco for openai gym is very meticulous and time consuming. Anyone who has tried installing mujoco would have faced some problem or the other. I had wasted so much time installing mujoco and the lack of documentation was a bit disappointing. A clear documentation on how to install it would have made the entire process so much easier and saved me a bunch of time. For anyone out there trying to install mujoco this is the documentation of the installation process that worked for me and my friends. The problem with getting mujoco (for openai gym) set up and ready for use is that certain versions of mujoco are only compatible with certain versions of python and certain versions of gym. There were two such combinations that seemed to work out. Installation Instructions: 1) Obtain the trial license from MuJoCo (For computer id download the file corresponding to your operating system. It is an executable file that needs permission to be executed. For Linux: copy the computer id and paste it in the box and get the license file emailed to your email id) 2) Download the MuJoCo pro <version: mjpro150 linux> 3) Unzip mjpro150 into a hidden folder named mujoco using : (Note: the folder should be in the home directory and must be named as shown) 4) Download and move mjkey.txt file that you received in your email into the folder .mujoco using : 5) nano ~/.bashrc 6) copy this line to your bashrc file 7) source ~/.bashrc 8) cd 9) Clone the mujoco-py repo (You may also choose to pip install, but this worked for all of us) 10) And then paste the lines below into your command window, PS: you should always be inside ~/mujoco-py folder to run the simulation. If you want to be able to import run it from anywhere then in step 10 instead of sudo python3 setup.py install use sudo python3 setup.py develop We are done with the installation. Now we need to test if we did it right. 11) Test Note: If you have Cuda setup and you have nvidia drivers setup on your Linux there\u2019s a chance of an openGL problem that you might encounter. You will face an error when you try to render the environment. If so adding this to the bash worked for me: Installation Instructions: 1) Obtain the trial license from MuJoCo (For computer id download the file corresponding to your operating system. It is an executable file that needs permission to be executed. For Linux: copy the computer id and paste it in the box and get the license file emailed to your email id) 2) Download the MuJoCo pro <version: mjpro131 linux> 3) Unzip mjpro131 into a hidden folder named mujoco using : (Note: the folder should be in the home directory and must be named as shown) 4) Download and move mjkey.txt file that you received in your email into the folder .mujoco using : 5) Then, You must install the exact same versions as above. If your default python version is python 3 pip install installs gym 0.10.4 in spite of explicitly mentioning 0.9.3. make sure you are using python 2.7. Note: You can also use the same license file for both the versions of mujoco You may also use a conda environment to do all this if you want. (Works!!) -- -- Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@ganeshprasanna/setting-up-mujoco-7a5ee62cf6dc"
        }
    },
    {
        "title": "Accelerated robot training through simulation in the cloud with ROS and Gazebo",
        "content": "Sign up Sign in Sign up Sign in V\u00edctor Mayoral Vilches Follow -- 1 Listen Share The content of this article comes from \u201crobot_gym: accelerated robot training through simulation in the cloud with ROS and Gazebo\u201d available at https://arxiv.org/pdf/1808.10369.pdf. Peer written with Alejandro Hern\u00e1ndez, Asier Bilbao Calvo, Irati Zamalloa Ugarte and Risto Kojcev. Rather than programming, training allows robots to achieve behaviors that generalize better and are capable to respond to real-world needs. However, such training requires a big amount of experimentation which is not always feasible for a physical robot. In this work, we present robot_gym, a framework to accelerate robot training through simulation in the cloud that makes use of roboticists\u2019 tools, simplifying the development and deployment processes on real robots. We unveil that, for simple tasks, simple 3DoF robots require more than 140 attempts to learn. For more complex, 6DoF robots, the number of attempts increases to more than 900 for the same task. We demonstrate that our framework, for simple tasks, accelerates the robot training time by more than 33% while maintaining similar levels of accuracy and repeatability. Reinforcement Learning (RL) has recently gained attention in the robotics field. Rather than programming, it allows roboticists to train robots, producing results that generalize better and are able to comply with the dynamic environments typically encountered in robotics. Furthermore, RL techniques, if used in combination with modular robotics, could empower a new generation of robots that are more adaptable and capable of performing a variety of tasks without human intervention [1]. While some results showed the feasibility of using RL in real robots [2], such approach is expensive, requires hundreds of thousands of attempts (performed by a group of robots) and a period of several months. These capabilities are only available to a restricted few, thereby training in simulation has gained popularity. The idea behind using simulation is to train a virtual model of the real robot until the desired behavior is learned and then transfer the knowledge to the real robot. The behavior can be further enhanced by exposing it to a restricted number of additional training iterations. Following some the initial releases of OpenAI\u2019s gym[3], many groups started using the Mujoco[4] physics engine. Others have used the Gazebo robot simulator[5] in combination with the Robot Operating System (ROS)[6] to create an environment with the common tools used by roboticists named gym_gazebo [7]. In this work, we introduce an extension of gym_gazebo, called robot_gym, that makes use of container technology to deploy experiments in a distributed way, accelerating the training process through a framework for distributed RL. We aim to provide answers to the following questions: By how much is it possible to accelerate robot training time with RL techniques? And: What is the associated cost of doing so? Our experimental results show a significant reduction of the training time. Compared to standard RL approaches, we achieve time reductions of up to 50% for simple tasks. Compared to standard RL approaches, we achieve time reductions of up to 50% for simple tasks. Most robots operate in a continuously changing environment which makes generalization of given tasks extremely hard. RL and, particularly, policy gradient methods, are among the techniques that allow for the development of more adaptive behaviors. However, even the most simple tasks demand long periods of training time. This aspect becomes specially relevant in robotics, where the time spent gathering experience from the environment (rollouts) is significantly more relevant than the time spent computing gradients or updating the corresponding policy that is being trained. To reduce the overall training time, this work proposes robot_gym, a framework for deploying robotic experiments in distributed workers, that aims to reduce the time spent gathering experience from the environment and, overall, reduce the training time of robots when using RL. Figure above pictures the architecture of the framework which has been inspired by previous work[8]. For a complete description of the framework, refer to the original article. To validate the framework, we ran experimental tests in simulation and deployed the results both in simulated and on real robots, obtaining similar results. The robots used in our experiments have been built using the H-ROS[9] technology, which simplifies the process of building, configuring and re-configuring robots. We experiment with two modular robots: a 3 Degrees-of-Freedom (DoF) robot in a SCARA configuration and a 6 DoF modular articulated arm. We analyze the impact of different number of workers, distributed through several machines and the number of iterations that the robot needs to converge. Our goal is to reach a specific position in the space stopping the training when the algorithm obtains zero as the target reward. Rewards are heuristically determined using the distance to the target point. During our experimentation, we use the Proximal Policy Optimization (PPO)[10], a state-of-the art policy gradient technique which alternates between sampling data through interaction with the environment and optimizing the \u201dsurrogate\u201d objective by clipping the policy probability ratio. We launched our experiment with 1, 2, 4 and 8 workers using 12 replicas. Within robot_gym, ray library is in charge of distributing the workers between the available replicas. The reward obtained during training is presented and pictured against the time it took in the figure above. Each curve illustrates the development of the same robot (modular robotic arm in a SCARA configuration with 3DoF) trained with the robot_gym framework, under a different number of workers distributed among the 12 available replicas. Using 1 worker, we can observe that the robot takes approximately 600 seconds to reach mean target reward of zero, that is, about 10 minutes of training time. Using 2 workers, the training time required to reach the target reward lowers to 400 seconds (6,6 minutes) approximately. When using 4, 8 or more workers, the training time required to reach a 0 target reward lowers to 300 seconds (5 minutes) approximately. From these results, we conclude that through the use of robot_gym framework and by distributing the rollout acquisition to a number of workers, we are able to reduce the training time by half in a simple robotics motion task. we are able to reduce the training time by half in a simple robotics motion task. The second picture depicted above illustrates the mean reward obtained versus the number of iterations required. With this plot, we aim to visualize the sample efficiency of using a variable number of workers. As it can be seen, the more workers we use, the less sample efficient it becomes. This remains an open problem we have observed through a variety of different experimental tests comprising different simple robotics tasks. We deployed the resulting global model denoted with \u03b8 into a) a simulated robot and b) a real robot. In both cases, the behavior displayed follows what was expected. The robots proceed to move their end effector towards the target point. Accuracy is calculated as the mean squared error between the target and the final end-effector when executing a trained RL network. The repeatability is calculated as mean square error between the mean from 10 experimental runs and the final end-effector position when executing a network. In this second experiment, we train in simulation a 6 DoF modular articulated arm, as shown in picture above. The objective remains similar: reach a given position in space. However, in this case, the robot includes additional degrees of freedom, making the search space much bigger; hence, the overall task more complicated, requiring additional training time. We launched our experiment with 1, 2, 4, 8, and 16 workers, using 12 replicas. From our experiments, we noted that this second scenario is much more sensitive to hyperparameters than the previous robot. Fine tuning was required for each different combination of workers, in order to make it converge towards the goal. Figure above displays the results of training the 6DoF robot with the robot_gym framework. For a single worker, the time required to train the model until the mean reward reaches zero is about 3000 seconds (50 minutes). Adding additional workers (4 and 8), reduces the time required to reach the target down to 2000 seconds (33.3 minutes), or reduces the training time by more than 33% compared to training only with a single worker. This paper introduced robot_gym, a framework to accelerate robot training using Gazebo and ROS in the cloud. Our work tackled the problem of how to accelerate the training time of robots using reinforcement learning techniques by distributing the load between several replicas in the cloud. We describe our method and display the impact of using different numbers of workers across replicas. We demonstrated with two different robots how the training time can be reduced by more than 33% in the worst case, while maintaining similar levels of accuracy. To the question how much does it cost to train a robot in the cloud?, our cloud solution provider charged us a total of 214,80 \u20ac for two weeks of experimentation work. 12 replicas were run on demand in parallel (only active if needed). 1603 hours of cloud computing was used in total (0,134 \u20ac/hour per instance or 1,606 \u20ac/hour for all the replicas running at the same time). The sample efficiency remains an open problem to tackle in future work. [1] V. Mayoral, R. Kojcev, N. Etxezarreta, A. Hernandez, and I. Zamalloa. Towards self-adaptable robots: from programming to training machines. ArXiv e-prints, Feb. 2018. [2] S. Levine, P. Pastor, A. Krizhevsky, and D. Quillen. Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection. ArXiv e-prints, Mar. 2016. [3] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym, 2016. [4] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026\u20135033. IEEE, 2012. [5] N. Koenig and A. Howard. Design and use paradigms for gazebo, an open-source multi-robot simulator. In Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International Conference on, volume 3, pages 2149\u20132154. IEEE, 2004. [6] M. Quigley, B. Gerkey, K. Conley, J. Faust, T. Foote, J. Leibs, E. Berger, R. Wheeler, and A. Ng. Ros: an open-source robot operating system. In Proc. of the IEEE Intl. Conf. on Robotics and Automation (ICRA) Workshop on Open Source Robotics, Kobe, Japan, May 2009. [7] I. Zamora, N. Gonzalez Lopez, V. Mayoral Vilches, and A. Hernandez Cordero. Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning using ROS and Gazebo. ArXiv e-prints, Aug. 2016. [8] E. Liang, R. Liaw, P. Moritz, R. Nishihara, R. Fox, K. Goldberg, J. E. Gonzalez, M. I. Jordan, and I. Stoica. Ray RLlib: A Framework for Distributed Reinforcement Learning. ArXiv eprints, Dec. 2017. [9] V. Mayoral, A. Hernandez, R. Kojcev, I. Muguruza, I. Zamalloa, A. Bilbao, and L. Usategi. \u00b4 The shift in the robotics paradigm: The hardware robot operating system (h-ros); an infrastructure to create interoperable robot components. In 2017 NASA/ESA Conference on Adaptive Hardware and Systems (AHS), pages 229\u2013236, July 2017. doi:10.1109/AHS.2017.8046383. [10] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. -- -- 1 Roboticist. Passionate about AI and Cybersecurity. Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@vmayoral/accelerated-robot-training-through-simulation-in-the-cloud-with-ros-and-gazebo-bac6bc493520"
        }
    },
    {
        "title": "Setting up ROS Kinetic and Gazebo 8 or 9",
        "content": "Sign up Sign in Sign up Sign in Abhik Singla Follow -- 3 Listen Share Robot Operating System (ROS) is a widely accepted meta operating system to develop and control a wide variety of the robots. In addition ROS supports Gazebo, a robotic physics-based simulator essential to prototype, debug and develop the control, planning and AI algorithms. Ubuntu 16.04 (Xenial) is used for this tutorial. ROS Kinetic installs Gazebo 7 by default so it is recommended to remove the previous installation. Proceed to the installation section if installing for the first time. Install the core of the ROS and rest of the packages can be added manually Run $ roscore to confirm ROS installation. Install Gazebo 8 or 9 using commands: Run $ gazebo to confirm Gazebo installation. Congratulations! now the system is configured with ROS Kinetic and Gazebo 8 or 9. github.com -- -- 3 Help Status About Careers Press Blog Privacy Terms Text to speech Teams",
        "source": "Medium",
        "metadata": {
            "url": "https://medium.com/@abhiksingla10/setting-up-ros-kinetic-and-gazebo-8-or-9-70f2231af21a"
        }
    }
]